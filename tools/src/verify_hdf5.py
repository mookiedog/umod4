#!/usr/bin/env python3
"""
HDF5 Log File Verification Script

This script inspects and validates HDF5 log files generated by decodelog.py.
It checks file structure, metadata, time progression, and data sanity.

Usage:
    python3 verify_hdf5.py <log_file.h5>
"""

import sys
import h5py
import numpy as np
from datetime import datetime, timezone

def print_header(text):
    """Print a formatted section header"""
    print(f"\n{'='*80}")
    print(f"  {text}")
    print(f"{'='*80}")

def print_subheader(text):
    """Print a formatted subsection header"""
    print(f"\n{'-'*80}")
    print(f"  {text}")
    print(f"{'-'*80}")

def verify_metadata(h5file):
    """Verify and display metadata attributes"""
    print_header("METADATA ATTRIBUTES")

    attrs = dict(h5file.attrs)

    # Required metadata
    required = ['log_version_ecu', 'log_version_ep', 'log_version_wp', 'log_start_timestamp_utc']

    missing = [key for key in required if key not in attrs]
    if missing:
        print(f"⚠️  WARNING: Missing required attributes: {missing}")

    # Display all attributes
    for key, value in sorted(attrs.items()):
        if key == 'log_start_timestamp_utc':
            # Convert Unix timestamp to human-readable
            try:
                dt = datetime.fromtimestamp(value, tz=timezone.utc)
                print(f"  {key:30s} = {value} ({dt.strftime('%Y-%m-%d %H:%M:%S.%f %Z')})")
            except:
                print(f"  {key:30s} = {value} (invalid timestamp)")
        else:
            print(f"  {key:30s} = {value}")

    return len(missing) == 0

def verify_eprom_loads(h5file):
    """Verify EPROM loads dataset"""
    print_subheader("EPROM LOADS")

    if 'eprom_loads' not in h5file:
        print("  ℹ️  No EPROM loads recorded")
        return True

    eprom = h5file['eprom_loads']
    print(f"  Number of EPROM loads: {len(eprom)}")

    if len(eprom) > 0:
        print("\n  EPROM Load Details:")
        for i, load in enumerate(eprom):
            name = load['name'].decode('utf-8').rstrip('\x00')
            addr = load['address']
            length = load['length']
            status = load['error_status']
            status_str = "✓ OK" if status == 0 else f"✗ ERROR ({status})"
            print(f"    [{i}] {name:20s} @ 0x{addr:04X}, len={length:5d}, {status_str}")

    return True

def verify_dataset(h5file, name, expected_cols=2, check_time=True):
    """Verify a single dataset"""
    if name not in h5file:
        return None

    ds = h5file[name]
    shape = ds.shape

    # Check if empty first
    if shape[0] == 0:
        print(f"  {name:30s}: EMPTY")
        return ds

    # Get data
    data = ds[:]

    # Check time monotonicity if applicable
    if check_time:
        if len(shape) == 2 and shape[1] >= 1:
            time_col = data[:, 0]
            if not np.all(np.diff(time_col) >= 0):
                print(f"  ⚠️  {name}: Time is NOT monotonically increasing!")
                # Find violations
                violations = np.where(np.diff(time_col) < 0)[0]
                print(f"      Found {len(violations)} time reversals at indices: {violations[:10]}")
        elif len(shape) == 1:
            # 1D array - check monotonicity
            if not np.all(np.diff(data) >= 0):
                print(f"  ⚠️  {name}: Time is NOT monotonically increasing!")
                violations = np.where(np.diff(data) < 0)[0]
                print(f"      Found {len(violations)} time reversals at indices: {violations[:10]}")

    # Display statistics
    if len(shape) == 2:
        if shape[1] == 2:
            val_col = data[:, 1]
            print(f"  {name:30s}: {shape[0]:6d} samples, "
                  f"value range [{np.min(val_col):10.3f}, {np.max(val_col):10.3f}], "
                  f"mean {np.mean(val_col):10.3f}")
        elif shape[1] == 3:  # GPS position
            print(f"  {name:30s}: {shape[0]:6d} samples, "
                  f"lat [{np.min(data[:, 1]):.6f}, {np.max(data[:, 1]):.6f}], "
                  f"lon [{np.min(data[:, 2]):.6f}, {np.max(data[:, 2]):.6f}]")
        else:
            # Other column counts
            print(f"  {name:30s}: {shape[0]:6d} samples, {shape[1]} columns")
    elif len(shape) == 1:  # 1D timestamp array
        print(f"  {name:30s}: {shape[0]:6d} timestamps, "
              f"range [{np.min(data)}, {np.max(data)}]")

    return ds

def verify_datasets(h5file):
    """Verify all datasets"""
    print_header("DATASETS")

    # List all datasets
    all_datasets = list(h5file.keys())
    print(f"\nTotal datasets: {len(all_datasets)}\n")

    # Group datasets by category for organized display
    timing_datasets = ['ecu_crankref_timestamp', 'ecu_crankref_id', 'ecu_camshaft_timestamp',
                      'ecu_cam_error', 'ecu_rpm_instantaneous', 'ecu_rpm_smoothed', 'ecu_time_marker']
    fuel_datasets = ['ecu_front_inj_on', 'ecu_front_inj_duration', 'ecu_rear_inj_on', 'ecu_rear_inj_duration']
    ignition_datasets = ['ecu_front_coil_on', 'ecu_front_coil_off', 'ecu_front_ign_delay',
                        'ecu_front_coil_manual_on', 'ecu_front_coil_manual_off',
                        'ecu_rear_coil_on', 'ecu_rear_coil_off', 'ecu_rear_ign_delay',
                        'ecu_rear_coil_manual_on', 'ecu_rear_coil_manual_off',
                        'ecu_spark_x1', 'ecu_spark_x2', 'ecu_nospark']
    sensor_datasets = ['ecu_throttle_adc', 'ecu_map_adc', 'ecu_aap_adc',
                      'ecu_air_temp_c', 'ecu_coolant_temp_c', 'ecu_battery_voltage_v']
    error_datasets = ['ecu_error_L000C', 'ecu_error_L000D', 'ecu_error_L000E', 'ecu_error_L000F']
    marker_datasets = ['ecu_marker_5ms', 'ecu_marker_p6_max']
    misc_datasets = ['ecu_cpu_event', 'ecu_l4000_event', 'ecu_portg_debug', 'ecu_fuel_pump']
    gps_datasets = ['gps_position', 'gps_velocity_mph', 'gps_fix_type']
    fs_datasets = ['wp_fs_write_time_ms', 'wp_fs_sync_time_ms']

    categories = [
        ("Engine Timing", timing_datasets),
        ("Fuel Injection", fuel_datasets),
        ("Ignition", ignition_datasets),
        ("Sensors", sensor_datasets),
        ("Errors", error_datasets),
        ("Markers", marker_datasets),
        ("GPS", gps_datasets),
        ("Filesystem", fs_datasets),
        ("Miscellaneous", misc_datasets)
    ]

    issues = []

    for category_name, dataset_list in categories:
        print_subheader(category_name)
        for ds_name in dataset_list:
            if ds_name == 'gps_position':
                ds = verify_dataset(h5file, ds_name, expected_cols=3)
            elif ds_name in ['ecu_marker_5ms', 'ecu_marker_p6_max']:
                ds = verify_dataset(h5file, ds_name, expected_cols=1, check_time=True)
            else:
                ds = verify_dataset(h5file, ds_name, expected_cols=2)

    # Check for any datasets we didn't categorize
    categorized = set([ds for cat in categories for ds in cat[1]]) | {'eprom_loads'}
    uncategorized = set(all_datasets) - categorized
    if uncategorized:
        print_subheader("Uncategorized Datasets")
        for ds_name in sorted(uncategorized):
            verify_dataset(h5file, ds_name)

    return True

def show_sample_data(h5file, dataset_name, num_samples=5):
    """Show sample data from a dataset"""
    if dataset_name not in h5file:
        print(f"  Dataset '{dataset_name}' not found")
        return

    ds = h5file[dataset_name]
    if ds.shape[0] == 0:
        print(f"  Dataset '{dataset_name}' is empty")
        return

    print(f"\n  First {num_samples} samples from '{dataset_name}':")
    data = ds[:min(num_samples, ds.shape[0])]

    if len(ds.shape) == 2:
        if ds.shape[1] == 2:
            print(f"    {'Time (ns)':>20s}  {'Value':>15s}")
            for row in data:
                print(f"    {int(row[0]):20d}  {row[1]:15.6f}")
        elif ds.shape[1] == 3:  # GPS position
            print(f"    {'Time (ns)':>20s}  {'Latitude':>12s}  {'Longitude':>12s}")
            for row in data:
                print(f"    {int(row[0]):20d}  {row[1]:12.6f}  {row[2]:12.6f}")
    elif len(ds.shape) == 1:
        print(f"    {'Time (ns)':>20s}")
        for val in data:
            print(f"    {int(val):20d}")

def verify_time_coverage(h5file):
    """Verify time coverage across all datasets"""
    print_header("TIME COVERAGE ANALYSIS")

    # Find datasets with data
    datasets_with_data = []
    for key in h5file.keys():
        if key == 'eprom_loads':
            continue
        ds = h5file[key]
        if ds.shape[0] > 0:
            # Get time range
            if len(ds.shape) == 2 and ds.shape[1] >= 1:
                time_data = ds[:, 0]
            elif len(ds.shape) == 1:
                time_data = ds[:]
            else:
                continue

            datasets_with_data.append((key, np.min(time_data), np.max(time_data), len(time_data)))

    if not datasets_with_data:
        print("  No datasets with time data found")
        return

    # Sort by first timestamp
    datasets_with_data.sort(key=lambda x: x[1])

    # Overall time range
    global_start = int(min(x[1] for x in datasets_with_data))
    global_end = int(max(x[2] for x in datasets_with_data))
    duration_sec = (global_end - global_start) / 1e9

    print(f"\n  Global time range:")
    print(f"    Start: {global_start:20d} ns")
    print(f"    End:   {global_end:20d} ns")
    print(f"    Duration: {duration_sec:.3f} seconds ({duration_sec/60:.2f} minutes)")

    print(f"\n  Dataset time ranges:")
    print(f"    {'Dataset':30s} {'Start (ns)':>20s} {'End (ns)':>20s} {'Samples':>10s} {'Span (s)':>10s}")
    for name, start, end, count in datasets_with_data:
        span = (end - start) / 1e9
        print(f"    {name:30s} {int(start):20d} {int(end):20d} {count:10d} {span:10.3f}")

def main():
    if len(sys.argv) != 2:
        print(f"Usage: {sys.argv[0]} <log_file.h5>")
        sys.exit(1)

    filename = sys.argv[1]

    print(f"\nVerifying HDF5 log file: {filename}")

    try:
        with h5py.File(filename, 'r') as h5file:
            # Run verification steps
            metadata_ok = verify_metadata(h5file)
            eprom_ok = verify_eprom_loads(h5file)
            datasets_ok = verify_datasets(h5file)
            verify_time_coverage(h5file)

            # Show some sample data
            print_header("SAMPLE DATA")

            # Find some datasets with data to show samples
            sample_datasets = ['ecu_rpm_smoothed', 'ecu_throttle_adc', 'ecu_coolant_temp_c',
                             'gps_position', 'ecu_marker_5ms']
            for ds_name in sample_datasets:
                if ds_name in h5file and h5file[ds_name].shape[0] > 0:
                    show_sample_data(h5file, ds_name, num_samples=3)

            # Summary
            print_header("VERIFICATION SUMMARY")
            print(f"  Metadata:  {'✓ PASS' if metadata_ok else '✗ FAIL'}")
            print(f"  EPROM:     {'✓ PASS' if eprom_ok else '✗ FAIL'}")
            print(f"  Datasets:  {'✓ PASS' if datasets_ok else '✗ FAIL'}")
            print(f"\n  Overall:   {'✓ VERIFICATION COMPLETE' if all([metadata_ok, eprom_ok, datasets_ok]) else '⚠️  ISSUES FOUND'}\n")

    except FileNotFoundError:
        print(f"ERROR: File not found: {filename}")
        sys.exit(1)
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()
